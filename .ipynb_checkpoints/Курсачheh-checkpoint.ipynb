{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e8eef5-0388-4537-84f3-04ba225ea000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 21:56:00.994000 12680 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, random, json, time, logging, psutil\n",
    "from pathlib import Path\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score,\n",
    "    brier_score_loss, accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca0c716-d0b7-44c9-b070-128b517ae431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Reproducibility ========================================================\n",
    "GLOBAL_SEED = 42\n",
    "def seed_everything(seed: int = GLOBAL_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "seed_everything()\n",
    "\n",
    "# === Configuration ==========================================================\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny\"\n",
    "NUM_LABELS = 2\n",
    "MAX_LEN = 128\n",
    "N_SPLITS = 5\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "OUTPUT_DIR = Path(\"./outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "MANUAL_PARAMS = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"batch_size\": 700,\n",
    "    \"epochs\": 4,\n",
    "}\n",
    "\n",
    "# === Logging ================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s — %(levelname)s — %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_resource_usage(step: str = \"\"):\n",
    "    cpu = psutil.cpu_percent()\n",
    "    ram = psutil.virtual_memory().percent\n",
    "    msg = f\"[RES] {step} — CPU: {cpu:.1f}%, RAM: {ram:.1f}%\"\n",
    "    if torch.cuda.is_available():\n",
    "        g = GPUtil.getGPUs()[0]\n",
    "        msg += f\", GPU-MEM: {g.memoryUtil*100:.1f}%\"\n",
    "    logger.info(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33fa5f67-ab23-4776-9d89-a014c4790b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Data ===================================================================\n",
    "RAW_DATA_FILE = Path(\"./data.parquet\")\n",
    "assert RAW_DATA_FILE.exists(), f\"{RAW_DATA_FILE} not found\"\n",
    "df1 = pd.read_parquet(RAW_DATA_FILE)\n",
    "assert {\"text\", \"target\"}.issubset(df.columns)\n",
    "df2 = pd.read_csv('vk_all_comments_cleaned.csv')\n",
    "df_full = pd.concat([df1, df2], ignore_index=True)\n",
    "df_full = df_full.drop_duplicates()\n",
    "\n",
    "# Дубликаты только по тексту\n",
    "df_full = df_full.drop_duplicates(subset='text')\n",
    "# Удалим строки, где текст пустой или только пробелы\n",
    "df_full = df_full[df_full['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Удалим строки с пропущенными значениями\n",
    "df_full = df_full.dropna(subset=['text', 'target'])\n",
    "df_full['target'] = df_full['target'].astype(int)\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # убрать лишние пробелы\n",
    "    text = re.sub(r'[^\\w\\sа-яё]', '', text)  # убрать спецсимволы, кроме букв и пробелов\n",
    "    return text.strip()\n",
    "\n",
    "df_full['text'] = df_full['text'].apply(clean_text)\n",
    "df = df_full\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Допустим, df_full уже предобработан\n",
    "df, df_test = train_test_split(\n",
    "    df_full,\n",
    "    test_size=0.1,            # 20% в тест\n",
    "    random_state=42,          # для воспроизводимости\n",
    "    stratify=df_full['target']  # сохраняем пропорции классов\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd11b8af-0a46-44b9-ae7a-c19337d5daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tokeniser ============================================================== \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=False,\n",
    "        )\n",
    "        enc[\"labels\"] = int(self.labels[idx])\n",
    "        return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a985f175-64ae-4198-9015-14ff44f4d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Metrics ================================================================\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
    "\n",
    "    try:\n",
    "        roc = roc_auc_score(labels, probs[:, 1])\n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"ROC-AUC failed: {e}\")\n",
    "        roc = float(\"nan\")\n",
    "\n",
    "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "    brier = brier_score_loss(labels, probs[:, 1])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\":    report[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\":        report[\"weighted avg\"][\"f1-score\"],\n",
    "        \"roc_auc\":   roc,\n",
    "        \"brier\":     brier,\n",
    "        \"precision_0\": report[\"0\"][\"precision\"],\n",
    "        \"recall_0\":    report[\"0\"][\"recall\"],\n",
    "        \"precision_1\": report[\"1\"][\"precision\"],\n",
    "        \"recall_1\":    report[\"1\"][\"recall\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddb91775-5c67-428a-8348-b14386421aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Custom Trainer with class weights ======================================\n",
    "def get_class_weights(labels):\n",
    "    w = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "    return torch.tensor(w, dtype=torch.float)\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights: torch.Tensor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights.to(self.args.device)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b1f65d6-bcca-4cc3-b2ab-91f28cf773fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Training fold ==========================================================\n",
    "def train_fold(train_idx, val_idx, params, fold_id=0):\n",
    "    log_resource_usage(f\"fold{fold_id}-start\")\n",
    "\n",
    "    train_ds = ToxicDataset(df.iloc[train_idx][\"text\"], df.iloc[train_idx][\"target\"])\n",
    "    val_ds   = ToxicDataset(df.iloc[val_idx][\"text\"],  df.iloc[val_idx][\"target\"])\n",
    "    class_wt = get_class_weights(df.iloc[train_idx][\"target\"].values)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR / f\"fold{fold_id}\",\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"batch_size\"],\n",
    "        num_train_epochs=params[\"epochs\"],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        seed=GLOBAL_SEED,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=NUM_LABELS\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_wt,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    log_resource_usage(f\"fold{fold_id}-end\")\n",
    "    return trainer, trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71984da4-e698-44eb-a64e-70d626e8e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:03:46,645 — INFO — [RES] start — CPU: 8.1%, RAM: 49.7%, GPU-MEM: 5.2%\n",
      "2025-06-19 22:03:46,663 — INFO — Fold 1 — Training\n",
      "2025-06-19 22:03:46,702 — INFO — [RES] fold1-start — CPU: 17.7%, RAM: 49.7%, GPU-MEM: 5.1%\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\887705796.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 06:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.310460</td>\n",
       "      <td>0.860060</td>\n",
       "      <td>0.891661</td>\n",
       "      <td>0.860060</td>\n",
       "      <td>0.868479</td>\n",
       "      <td>0.942872</td>\n",
       "      <td>0.099656</td>\n",
       "      <td>0.963933</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.608387</td>\n",
       "      <td>0.874405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293100</td>\n",
       "      <td>0.269444</td>\n",
       "      <td>0.880383</td>\n",
       "      <td>0.904506</td>\n",
       "      <td>0.880383</td>\n",
       "      <td>0.886778</td>\n",
       "      <td>0.957326</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>0.969432</td>\n",
       "      <td>0.877536</td>\n",
       "      <td>0.650028</td>\n",
       "      <td>0.891544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.255291</td>\n",
       "      <td>0.885841</td>\n",
       "      <td>0.909072</td>\n",
       "      <td>0.885841</td>\n",
       "      <td>0.891878</td>\n",
       "      <td>0.962082</td>\n",
       "      <td>0.083972</td>\n",
       "      <td>0.972506</td>\n",
       "      <td>0.881641</td>\n",
       "      <td>0.660440</td>\n",
       "      <td>0.902304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.253145</td>\n",
       "      <td>0.885416</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.885416</td>\n",
       "      <td>0.891609</td>\n",
       "      <td>0.963087</td>\n",
       "      <td>0.084348</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.879989</td>\n",
       "      <td>0.658415</td>\n",
       "      <td>0.906684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:10:36,252 — INFO — [RES] fold1-end — CPU: 42.5%, RAM: 49.8%, GPU-MEM: 79.2%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:10:49,365 — INFO — Fold1 metrics: {\n",
      "  \"eval_loss\": 0.2552907168865204,\n",
      "  \"eval_accuracy\": 0.8858414787573793,\n",
      "  \"eval_precision\": 0.9090719700377062,\n",
      "  \"eval_recall\": 0.8858414787573793,\n",
      "  \"eval_f1\": 0.8918783237998188,\n",
      "  \"eval_roc_auc\": 0.9620823407272072,\n",
      "  \"eval_brier\": 0.08397178981412581,\n",
      "  \"eval_precision_0\": 0.9725058284428009,\n",
      "  \"eval_recall_0\": 0.8816412797901028,\n",
      "  \"eval_precision_1\": 0.6604404795093393,\n",
      "  \"eval_recall_1\": 0.9023043229860979,\n",
      "  \"eval_runtime\": 13.112,\n",
      "  \"eval_samples_per_second\": 3940.276,\n",
      "  \"eval_steps_per_second\": 5.644,\n",
      "  \"epoch\": 4.0\n",
      "}\n",
      "2025-06-19 22:10:49,367 — INFO — Fold 2 — Training\n",
      "2025-06-19 22:10:49,443 — INFO — [RES] fold2-start — CPU: 54.7%, RAM: 49.7%, GPU-MEM: 79.4%\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\887705796.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 06:35, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.310318</td>\n",
       "      <td>0.856324</td>\n",
       "      <td>0.890619</td>\n",
       "      <td>0.856324</td>\n",
       "      <td>0.865318</td>\n",
       "      <td>0.943867</td>\n",
       "      <td>0.102531</td>\n",
       "      <td>0.964713</td>\n",
       "      <td>0.850788</td>\n",
       "      <td>0.600208</td>\n",
       "      <td>0.878023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.267644</td>\n",
       "      <td>0.876706</td>\n",
       "      <td>0.904433</td>\n",
       "      <td>0.876706</td>\n",
       "      <td>0.883794</td>\n",
       "      <td>0.958777</td>\n",
       "      <td>0.089756</td>\n",
       "      <td>0.972026</td>\n",
       "      <td>0.870296</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.901828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.253895</td>\n",
       "      <td>0.881525</td>\n",
       "      <td>0.908430</td>\n",
       "      <td>0.881525</td>\n",
       "      <td>0.888280</td>\n",
       "      <td>0.963353</td>\n",
       "      <td>0.086490</td>\n",
       "      <td>0.974772</td>\n",
       "      <td>0.873916</td>\n",
       "      <td>0.648398</td>\n",
       "      <td>0.911350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.247330</td>\n",
       "      <td>0.888745</td>\n",
       "      <td>0.911363</td>\n",
       "      <td>0.888745</td>\n",
       "      <td>0.894573</td>\n",
       "      <td>0.964519</td>\n",
       "      <td>0.081615</td>\n",
       "      <td>0.973906</td>\n",
       "      <td>0.884046</td>\n",
       "      <td>0.666224</td>\n",
       "      <td>0.907161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:17:37,533 — INFO — [RES] fold2-end — CPU: 42.2%, RAM: 50.1%, GPU-MEM: 81.1%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:17:50,770 — INFO — Fold2 metrics: {\n",
      "  \"eval_loss\": 0.24732965230941772,\n",
      "  \"eval_accuracy\": 0.8887447982192974,\n",
      "  \"eval_precision\": 0.9113631534056748,\n",
      "  \"eval_recall\": 0.8887447982192974,\n",
      "  \"eval_f1\": 0.8945727850044658,\n",
      "  \"eval_roc_auc\": 0.9645189947727821,\n",
      "  \"eval_brier\": 0.08161547924357343,\n",
      "  \"eval_precision_0\": 0.9739060618225612,\n",
      "  \"eval_recall_0\": 0.8840463523066832,\n",
      "  \"eval_precision_1\": 0.6662237762237763,\n",
      "  \"eval_recall_1\": 0.907160540849362,\n",
      "  \"eval_runtime\": 13.2372,\n",
      "  \"eval_samples_per_second\": 3903.005,\n",
      "  \"eval_steps_per_second\": 5.59,\n",
      "  \"epoch\": 4.0\n",
      "}\n",
      "2025-06-19 22:17:50,772 — INFO — Fold 3 — Training\n",
      "2025-06-19 22:17:50,822 — INFO — [RES] fold3-start — CPU: 53.4%, RAM: 49.8%, GPU-MEM: 81.2%\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\887705796.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 06:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.312066</td>\n",
       "      <td>0.862015</td>\n",
       "      <td>0.891670</td>\n",
       "      <td>0.862015</td>\n",
       "      <td>0.870049</td>\n",
       "      <td>0.942502</td>\n",
       "      <td>0.099422</td>\n",
       "      <td>0.962695</td>\n",
       "      <td>0.860141</td>\n",
       "      <td>0.613287</td>\n",
       "      <td>0.869358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.268037</td>\n",
       "      <td>0.881738</td>\n",
       "      <td>0.905213</td>\n",
       "      <td>0.881738</td>\n",
       "      <td>0.887977</td>\n",
       "      <td>0.957897</td>\n",
       "      <td>0.086690</td>\n",
       "      <td>0.969514</td>\n",
       "      <td>0.879212</td>\n",
       "      <td>0.653181</td>\n",
       "      <td>0.891640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.254831</td>\n",
       "      <td>0.884429</td>\n",
       "      <td>0.908682</td>\n",
       "      <td>0.884429</td>\n",
       "      <td>0.890676</td>\n",
       "      <td>0.962357</td>\n",
       "      <td>0.084593</td>\n",
       "      <td>0.972985</td>\n",
       "      <td>0.879358</td>\n",
       "      <td>0.656641</td>\n",
       "      <td>0.904304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.250324</td>\n",
       "      <td>0.888358</td>\n",
       "      <td>0.910622</td>\n",
       "      <td>0.888358</td>\n",
       "      <td>0.894148</td>\n",
       "      <td>0.963526</td>\n",
       "      <td>0.081746</td>\n",
       "      <td>0.973005</td>\n",
       "      <td>0.884411</td>\n",
       "      <td>0.666105</td>\n",
       "      <td>0.903828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:24:40,925 — INFO — [RES] fold3-end — CPU: 42.8%, RAM: 49.9%, GPU-MEM: 82.7%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:24:54,469 — INFO — Fold3 metrics: {\n",
      "  \"eval_loss\": 0.25032371282577515,\n",
      "  \"eval_accuracy\": 0.8883576889577083,\n",
      "  \"eval_precision\": 0.9106215428103798,\n",
      "  \"eval_recall\": 0.8883576889577083,\n",
      "  \"eval_f1\": 0.894147698033348,\n",
      "  \"eval_roc_auc\": 0.9635260659494129,\n",
      "  \"eval_brier\": 0.08174590258233794,\n",
      "  \"eval_precision_0\": 0.973005479085928,\n",
      "  \"eval_recall_0\": 0.8844107572334378,\n",
      "  \"eval_precision_1\": 0.6661052631578948,\n",
      "  \"eval_recall_1\": 0.9038278423157494,\n",
      "  \"eval_runtime\": 13.5427,\n",
      "  \"eval_samples_per_second\": 3814.976,\n",
      "  \"eval_steps_per_second\": 5.464,\n",
      "  \"epoch\": 4.0\n",
      "}\n",
      "2025-06-19 22:24:54,471 — INFO — Fold 4 — Training\n",
      "2025-06-19 22:24:54,521 — INFO — [RES] fold4-start — CPU: 53.3%, RAM: 49.8%, GPU-MEM: 82.5%\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\887705796.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 06:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.318966</td>\n",
       "      <td>0.842950</td>\n",
       "      <td>0.887054</td>\n",
       "      <td>0.842950</td>\n",
       "      <td>0.853975</td>\n",
       "      <td>0.942304</td>\n",
       "      <td>0.111637</td>\n",
       "      <td>0.967098</td>\n",
       "      <td>0.831163</td>\n",
       "      <td>0.573279</td>\n",
       "      <td>0.889153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283300</td>\n",
       "      <td>0.269556</td>\n",
       "      <td>0.877422</td>\n",
       "      <td>0.904174</td>\n",
       "      <td>0.877422</td>\n",
       "      <td>0.884338</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.089129</td>\n",
       "      <td>0.971109</td>\n",
       "      <td>0.872097</td>\n",
       "      <td>0.641788</td>\n",
       "      <td>0.898295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.256936</td>\n",
       "      <td>0.883732</td>\n",
       "      <td>0.908987</td>\n",
       "      <td>0.883732</td>\n",
       "      <td>0.890156</td>\n",
       "      <td>0.962092</td>\n",
       "      <td>0.085822</td>\n",
       "      <td>0.974006</td>\n",
       "      <td>0.877490</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>0.908199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.253356</td>\n",
       "      <td>0.885532</td>\n",
       "      <td>0.910307</td>\n",
       "      <td>0.885532</td>\n",
       "      <td>0.891809</td>\n",
       "      <td>0.963178</td>\n",
       "      <td>0.084380</td>\n",
       "      <td>0.974735</td>\n",
       "      <td>0.879118</td>\n",
       "      <td>0.657748</td>\n",
       "      <td>0.910675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:31:45,451 — INFO — [RES] fold4-end — CPU: 43.0%, RAM: 43.0%, GPU-MEM: 85.1%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:31:58,962 — INFO — Fold4 metrics: {\n",
      "  \"eval_loss\": 0.2533564269542694,\n",
      "  \"eval_accuracy\": 0.885531791348108,\n",
      "  \"eval_precision\": 0.9103066241273656,\n",
      "  \"eval_recall\": 0.885531791348108,\n",
      "  \"eval_f1\": 0.8918094861762629,\n",
      "  \"eval_roc_auc\": 0.963178006303586,\n",
      "  \"eval_brier\": 0.08438020694395856,\n",
      "  \"eval_precision_0\": 0.9747346872811506,\n",
      "  \"eval_recall_0\": 0.8791176756389077,\n",
      "  \"eval_precision_1\": 0.657748125730793,\n",
      "  \"eval_recall_1\": 0.9106751737929721,\n",
      "  \"eval_runtime\": 13.51,\n",
      "  \"eval_samples_per_second\": 3824.208,\n",
      "  \"eval_steps_per_second\": 5.477,\n",
      "  \"epoch\": 4.0\n",
      "}\n",
      "2025-06-19 22:31:58,965 — INFO — Fold 5 — Training\n",
      "2025-06-19 22:31:59,003 — INFO — [RES] fold5-start — CPU: 53.1%, RAM: 42.8%, GPU-MEM: 84.7%\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\887705796.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 06:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.313133</td>\n",
       "      <td>0.852276</td>\n",
       "      <td>0.890057</td>\n",
       "      <td>0.852276</td>\n",
       "      <td>0.861962</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.105274</td>\n",
       "      <td>0.966265</td>\n",
       "      <td>0.844059</td>\n",
       "      <td>0.591329</td>\n",
       "      <td>0.884487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.267290</td>\n",
       "      <td>0.881871</td>\n",
       "      <td>0.906055</td>\n",
       "      <td>0.881871</td>\n",
       "      <td>0.888215</td>\n",
       "      <td>0.957977</td>\n",
       "      <td>0.086659</td>\n",
       "      <td>0.970756</td>\n",
       "      <td>0.878192</td>\n",
       "      <td>0.652433</td>\n",
       "      <td>0.896296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.260633</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906299</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.882658</td>\n",
       "      <td>0.962526</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.976050</td>\n",
       "      <td>0.864320</td>\n",
       "      <td>0.632880</td>\n",
       "      <td>0.916865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.250299</td>\n",
       "      <td>0.889149</td>\n",
       "      <td>0.911636</td>\n",
       "      <td>0.889149</td>\n",
       "      <td>0.894942</td>\n",
       "      <td>0.963638</td>\n",
       "      <td>0.082064</td>\n",
       "      <td>0.974022</td>\n",
       "      <td>0.884459</td>\n",
       "      <td>0.667087</td>\n",
       "      <td>0.907533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:38:50,319 — INFO — [RES] fold5-end — CPU: 42.3%, RAM: 43.5%, GPU-MEM: 86.6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:39:03,711 — INFO — Fold5 metrics: {\n",
      "  \"eval_loss\": 0.250299334526062,\n",
      "  \"eval_accuracy\": 0.8891491173738,\n",
      "  \"eval_precision\": 0.9116357781828284,\n",
      "  \"eval_recall\": 0.8891491173738,\n",
      "  \"eval_f1\": 0.8949416093025763,\n",
      "  \"eval_roc_auc\": 0.9636376595324758,\n",
      "  \"eval_brier\": 0.08206380423211311,\n",
      "  \"eval_precision_0\": 0.9740221520680614,\n",
      "  \"eval_recall_0\": 0.884459344557005,\n",
      "  \"eval_precision_1\": 0.6670866582668347,\n",
      "  \"eval_recall_1\": 0.9075326159413389,\n",
      "  \"eval_runtime\": 13.3912,\n",
      "  \"eval_samples_per_second\": 3858.057,\n",
      "  \"eval_steps_per_second\": 5.526,\n",
      "  \"epoch\": 4.0\n",
      "}\n",
      "2025-06-19 22:39:03,712 — INFO — CV-avg: {\n",
      "  \"eval_loss\": 0.25131996870040896,\n",
      "  \"eval_accuracy\": 0.8875249749312587,\n",
      "  \"eval_precision\": 0.9105998137127909,\n",
      "  \"eval_recall\": 0.8875249749312587,\n",
      "  \"eval_f1\": 0.8934699804632945,\n",
      "  \"eval_roc_auc\": 0.9633886134570929,\n",
      "  \"eval_brier\": 0.08275543656322178,\n",
      "  \"eval_precision_0\": 0.9736348417401004,\n",
      "  \"eval_recall_0\": 0.8827350819052272,\n",
      "  \"eval_precision_1\": 0.6635208605777276,\n",
      "  \"eval_recall_1\": 0.906300099177104,\n",
      "  \"eval_runtime\": 13.358619999999998,\n",
      "  \"eval_samples_per_second\": 3868.1044,\n",
      "  \"eval_steps_per_second\": 5.5402000000000005,\n",
      "  \"epoch\": 4.0\n",
      "}\n",
      "2025-06-19 22:39:03,769 — INFO — Saved best model to C:\\Users\\narumaru\\workspace\\nlp-projekt\\best_cointegrated_rubert-tiny\n",
      "2025-06-19 22:39:03,811 — INFO — [RES] end — CPU: 53.4%, RAM: 43.2%, GPU-MEM: 86.4%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# === Main ===================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    log_resource_usage(\"start\")\n",
    "    best_params = MANUAL_PARAMS\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    all_metrics, trainers = [], []\n",
    "    for k, (tr, va) in enumerate(skf.split(df, df[\"target\"]), 1):\n",
    "        logger.info(f\"Fold {k} — Training\")\n",
    "        t, m = train_fold(tr, va, best_params, fold_id=k)\n",
    "        trainers.append(t)\n",
    "        all_metrics.append(m)\n",
    "        logger.info(f\"Fold{k} metrics: {json.dumps(m, indent=2)}\")\n",
    "\n",
    "    avg = {k: float(np.mean([m[k] for m in all_metrics])) for k in all_metrics[0]}\n",
    "    logger.info(f\"CV-avg: {json.dumps(avg, indent=2)}\")\n",
    "\n",
    "    best_i = int(np.argmax([m.get(\"f1\", 0.0) for m in all_metrics]))\n",
    "    best_dir = Path(f\"./best_{MODEL_NAME.replace('/', '_')}\")\n",
    "    trainers[best_i].save_model(best_dir)\n",
    "    tokenizer.save_pretrained(best_dir)\n",
    "    logger.info(f\"Saved best model to {best_dir.resolve()}\")\n",
    "\n",
    "    log_resource_usage(\"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad7c26-f3f5-4b9f-807e-99bf1faca25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96b22c62-f32d-4182-ae3f-3383b557824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ToxicDataset(df_test[\"text\"], df_test[\"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "35673fe9-7ed0-4113-9209-889b285de3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29564, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_path = Path(f\"./best_{MODEL_NAME.replace('/', '_')}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "71a9ac06-aeab-4459-8395-5794191a2b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\848745646.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  test_trainer = Trainer(\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_args = TrainingArguments(\n",
    "    output_dir=\"./test_eval\",\n",
    "    per_device_eval_batch_size=MANUAL_PARAMS[\"batch_size\"],\n",
    "    dataloader_drop_last=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dab47008-df7d-4beb-a620-0ece56554523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.2732057273387909,\n",
      "  \"eval_model_preparation_time\": 0.001,\n",
      "  \"eval_accuracy\": 0.8880604814827718,\n",
      "  \"eval_precision\": 0.9095881329023737,\n",
      "  \"eval_recall\": 0.8880604814827718,\n",
      "  \"eval_f1\": 0.8937491054339433,\n",
      "  \"eval_roc_auc\": 0.9621596178067646,\n",
      "  \"eval_brier\": 0.0826568354974826,\n",
      "  \"eval_precision_0\": 0.9715478361001824,\n",
      "  \"eval_recall_0\": 0.8854344308889763,\n",
      "  \"eval_precision_1\": 0.6667090700928635,\n",
      "  \"eval_recall_1\": 0.8983544737744258,\n",
      "  \"eval_runtime\": 7.7011,\n",
      "  \"eval_samples_per_second\": 3727.15,\n",
      "  \"eval_steps_per_second\": 5.454\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(json.dumps(test_metrics, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8001f7d4-3837-4222-b640-511942e626ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0657805b-9789-412a-ac1d-fda5e68a601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\848745646.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  test_trainer = Trainer(\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_args = TrainingArguments(\n",
    "    output_dir=\"./test_eval\",\n",
    "    per_device_eval_batch_size=MANUAL_PARAMS[\"batch_size\"],\n",
    "    dataloader_drop_last=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2906ae55-3e80-4dc8-9d6d-3fd6db1f0303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.6615400314331055,\n",
      "  \"eval_model_preparation_time\": 0.001,\n",
      "  \"eval_accuracy\": 0.7961537121555238,\n",
      "  \"eval_precision\": 0.6898066001549373,\n",
      "  \"eval_recall\": 0.7961537121555238,\n",
      "  \"eval_f1\": 0.7069802347927108,\n",
      "  \"eval_roc_auc\": 0.5757318707812678,\n",
      "  \"eval_brier\": 0.23421584618645075,\n",
      "  \"eval_precision_0\": 0.7968324844763832,\n",
      "  \"eval_recall_0\": 0.9988193624557261,\n",
      "  \"eval_precision_1\": 0.2702702702702703,\n",
      "  \"eval_recall_1\": 0.0017140898183064792,\n",
      "  \"eval_runtime\": 10.6054,\n",
      "  \"eval_samples_per_second\": 2706.439,\n",
      "  \"eval_steps_per_second\": 3.96\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(json.dumps(test_metrics, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "09bcf930-0ec0-492e-bc9d-408612ec1b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a31e3530-b97e-4ed9-bd42-9c351d45cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narumaru\\AppData\\Local\\Temp\\ipykernel_12680\\848745646.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  test_trainer = Trainer(\n",
      "C:\\Users\\narumaru\\anaconda3\\envs\\resnet\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_args = TrainingArguments(\n",
    "    output_dir=\"./test_eval\",\n",
    "    per_device_eval_batch_size=MANUAL_PARAMS[\"batch_size\"],\n",
    "    dataloader_drop_last=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2e44b386-514d-4657-a998-253d88b87fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"eval_loss\": 0.7486100196838379,\n",
      "  \"eval_model_preparation_time\": 0.003,\n",
      "  \"eval_accuracy\": 0.2145071943699265,\n",
      "  \"eval_precision\": 0.6172728339212286,\n",
      "  \"eval_recall\": 0.2145071943699265,\n",
      "  \"eval_f1\": 0.10305667919250974,\n",
      "  \"eval_roc_auc\": 0.4442779823307763,\n",
      "  \"eval_brier\": 0.2775839798554892,\n",
      "  \"eval_precision_0\": 0.7233748271092669,\n",
      "  \"eval_recall_0\": 0.02286938650575014,\n",
      "  \"eval_precision_1\": 0.20135811293781272,\n",
      "  \"eval_recall_1\": 0.9657182036338704,\n",
      "  \"eval_runtime\": 19.2696,\n",
      "  \"eval_samples_per_second\": 1489.551,\n",
      "  \"eval_steps_per_second\": 2.18\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test_trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(json.dumps(test_metrics, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5adf97-026e-4d9d-809b-535ba005752d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "861b8b07-f1e6-42a6-87f8-e26c164e47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    RocCurveDisplay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "721b8bf4-dff0-4a40-bd5f-4ae0575e9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания\n",
    "preds_output = test_trainer.predict(test_dataset)\n",
    "logits = preds_output.predictions\n",
    "pred_labels = np.argmax(logits, axis=1)\n",
    "probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "true_labels = df_test[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1510131a-3052-4867-899b-830512579217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[REPORT]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9715    0.8854    0.9265     22869\n",
      "           1     0.6667    0.8984    0.7654      5834\n",
      "\n",
      "    accuracy                         0.8881     28703\n",
      "   macro avg     0.8191    0.8919    0.8459     28703\n",
      "weighted avg     0.9096    0.8881    0.8937     28703\n",
      "\n",
      "ROC-AUC: 0.9622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\n[REPORT]\")\n",
    "print(classification_report(true_labels, pred_labels, digits=4))\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(true_labels, probs[:, 1])\n",
    "    print(f\"ROC-AUC: {auc_score:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"ROC-AUC: невозможно посчитать (один класс?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a1360fe-15a7-4aa8-bb88-610441210e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"neg\", \"pos\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix on Test\")\n",
    "plt.savefig(\"confusion_matrix_test.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e93755d2-b272-45a9-93fc-7cc73e5035e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(true_labels, probs[:, 1])\n",
    "roc_disp = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "roc_disp.plot()\n",
    "plt.title(f\"ROC Curve (AUC={auc_score:.4f})\")\n",
    "plt.savefig(\"roc_curve_test.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bb47355-262b-4386-8511-3e33c78bf81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено в test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "df_preds = df_test.copy()\n",
    "df_preds[\"pred\"] = pred_labels\n",
    "df_preds[\"prob_0\"] = probs[:, 0]\n",
    "df_preds[\"prob_1\"] = probs[:, 1]\n",
    "df_preds.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Сохранено в test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6dfb710-c429-412d-b31f-8c0ad99eaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.read_csv(\"test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cc7f327e-bf73-42d4-bbfd-ad718695ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Топ-10 самых неуверенных предсказаний:\n",
      "\n",
      "🟡 Index: 22325\n",
      "Text: гиены...\n",
      "True: 1 | Pred: 0 | Prob_1: 0.500\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 17536\n",
      "Text: а зелень кладёмхрен укроп и тд...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.500\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 23459\n",
      "Text: михаил тимофеич ещё и избыточные зазоры в конструкцию заложил с избыточным газовым двигателемвот и будет стрелятьпобывав в любом говнище а споры насчёт штурмгевера и калаша для лошарздесь я с вами цел...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.500\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 551\n",
      "Text: возвращаемся к мужикам с долгами...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 12536\n",
      "Text: синенькую канешна...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 20093\n",
      "Text: читаю и офигеваю как как можно было брать собаку не познакомившись с ней зачем мучать животное так вы не подружитесь собака испытывает стресс плюс агрессия если он сорвется это будет катастрофа я взял...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 24842\n",
      "Text: не нету пилы...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 26460\n",
      "Text: и ты год играешь такой ретинг не каго нет без денгами и без левак что паднимают ретинг...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 22512\n",
      "Text: у тебя сколько лошадиных сил  у меня 300 арбузосил...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🟡 Index: 18988\n",
      "Text: жадность девочку сгубила...\n",
      "True: 0 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_preds[\"confidence\"] = np.abs(df_preds[\"prob_1\"] - 0.5)\n",
    "uncertain = df_preds.sort_values(\"confidence\").head(10)\n",
    "\n",
    "print(\"🔍 Топ-10 самых неуверенных предсказаний:\\n\")\n",
    "for i, row in uncertain.iterrows():\n",
    "    print(f\"🟡 Index: {i}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\")\n",
    "    print(f\"True: {row['target']} | Pred: {row['pred']} | Prob_1: {row['prob_1']:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19320fa1-c287-45ec-9772-73ae6b5a2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Топ-10 ошибок модели:\n",
      "\n",
      "🔴 Index: 22325\n",
      "Text: гиены...\n",
      "True: 1 | Pred: 0 | Prob_1: 0.500\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 20093\n",
      "Text: читаю и офигеваю как как можно было брать собаку не познакомившись с ней зачем мучать животное так вы не подружитесь собака испытывает стресс плюс агрессия если он сорвется это будет катастрофа я взял...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 17297\n",
      "Text: он молодецон постоянно же даёт стимул народу восстатьа народ то только обсуждает...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 23587\n",
      "Text: тут мне не понятно пятак это твоё русское а мои доллары сша не путай...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 4694\n",
      "Text: 15 лет строгого и это мягко потому что и наркота и убийство...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 5254\n",
      "Text: к стенке...\n",
      "True: 1 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 20627\n",
      "Text: нам надо такую амброзию косить...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 13751\n",
      "Text: кто это сочиняет  посадить бы таких на среднестатистические пенсии и зарплаты и пусть до конца жизни гордятся президентом...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.501\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 22614\n",
      "Text: всех братьев вон из калининграда что они там вообще делают тараканы...\n",
      "True: 1 | Pred: 0 | Prob_1: 0.499\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 28277\n",
      "Text: каждая жена обязана кормить мужа потому что женевской согласно каждый конвенции имеет пленный право на горячую еду...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.502\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "errors = df_preds[df_preds[\"pred\"] != df_preds[\"target\"]]\n",
    "errors = errors.copy()\n",
    "errors[\"confidence\"] = np.abs(errors[\"prob_1\"] - 0.5)\n",
    "errors = errors.sort_values(\"confidence\")  # можно сортировать по наименьшей уверенности\n",
    "errors_top = errors.head(10)\n",
    "\n",
    "print(\"❌ Топ-10 ошибок модели:\\n\")\n",
    "for i, row in errors_top.iterrows():\n",
    "    print(f\"🔴 Index: {i}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\")\n",
    "    print(f\"True: {row['target']} | Pred: {row['pred']} | Prob_1: {row['prob_1']:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "677bb6a3-d677-45f1-85f6-1ef8ef50d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уверенность — насколько prob_1 отличается от 0.5 (т.е. уверенность в классе 1)\n",
    "df_preds[\"confidence\"] = np.abs(df_preds[\"prob_1\"] - 0.5)\n",
    "\n",
    "# Ошибки модели\n",
    "errors = df_preds[df_preds[\"pred\"] != df_preds[\"target\"]]\n",
    "\n",
    "# Уверенные ошибки: сортируем по убыванию уверенности\n",
    "confident_errors = errors.sort_values(\"confidence\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f3328bd-5cec-4669-aae3-f2548e24a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Топ-10 уверенных, но ошибочных предсказаний:\n",
      "\n",
      "🔴 Index: 16661\n",
      "Text: вдудь и вся его кодла пускай пиздячат на хуй...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.995 | Confidence: 0.495\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 25916\n",
      "Text: ты чо нивстиме значит пидоры нахуй их гогверсия и всё такое нахуй эпик сторе скачаю с торрента...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.995 | Confidence: 0.495\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 8665\n",
      "Text: н е сказал бы понася могут ругать как пиздоболы так и нет а вот если человек хвалит понася  тут к гадалке не ходи либо дурак либо пиздобол...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.994 | Confidence: 0.494\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 27994\n",
      "Text: бей фидфрлусшв видишль ребятя быют ншги пидр е не буд рлвнодшена ты им пмшги о...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.994 | Confidence: 0.494\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 6505\n",
      "Text: может просто социальная несправедливость не дает покоя предыдущему докладчику время ведь сейчас такое ты либо работаешь клоуном у пидорасов либо пидорасом у клоунов за тот же самый мелкий прайс либо в...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.994 | Confidence: 0.494\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 15111\n",
      "Text: эй михалыч там пожарка пищит да хуй с ней пущай пищит смори чо тут у французов эй михалыч там дым какойто да мало ли само загорелось само потухнет ты закрой выход запасной пущай через центральный пизд...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.992 | Confidence: 0.492\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 6273\n",
      "Text: звиздеж у меня под окнами такая окна простые советские и норм вот когда по ночам бухие пидоры орут тогда да...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.992 | Confidence: 0.492\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 13943\n",
      "Text: хм помоему ты не слышал как кричит такой поросенок его хуй выпиздишь из загона сначала хоть за уши хоть сапогами а ещё эту тушу надо протащить метров 40 и завести в станок  и только потом ты залезешь ...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.991 | Confidence: 0.491\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 14152\n",
      "Text: и таак опять пляж  емое  да он нудистский...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.991 | Confidence: 0.491\n",
      "--------------------------------------------------------------------------------\n",
      "🔴 Index: 23838\n",
      "Text: апсолютно согласен с емельянычем наши 12310 рублей намного круче всей этой западной херни там вокруг одни пидоы...\n",
      "True: 0 | Pred: 1 | Prob_1: 0.991 | Confidence: 0.491\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"❗ Топ-10 уверенных, но ошибочных предсказаний:\\n\")\n",
    "for i, row in confident_errors.iterrows():\n",
    "    print(f\"🔴 Index: {i}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\")\n",
    "    print(f\"True: {row['target']} | Pred: {row['pred']} | Prob_1: {row['prob_1']:.3f} | Confidence: {row['confidence']:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c430993-e166-4b23-a0e3-f2f199d047e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29564, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import LayerIntegratedGradients\n",
    "import torch\n",
    "\n",
    "# Убедись, что модель в eval-режиме\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1d0ac5b5-7666-4813-9b75-3e5d9282a14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Текст для интерпретации:\n",
      " апсолютно согласен с емельянычем наши 12310 рублей намного круче всей этой западной херни там вокруг одни пидоы\n"
     ]
    }
   ],
   "source": [
    "toxic_text = confident_errors.iloc[9][\"text\"]\n",
    "print(\"🔍 Текст для интерпретации:\\n\", toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4c4a28b2-ef74-4ae5-ad53-f0bc57008f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_toxic(input_ids, attention_mask):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return out.logits[:, 1]  # логит класса 1\n",
    "\n",
    "lig = LayerIntegratedGradients(forward_toxic, model.bert.embeddings.word_embeddings)\n",
    "device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bdcb64b8-100d-4393-bd1c-53ce612e2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_toxic = tokenizer(\n",
    "    toxic_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    ")\n",
    "enc_toxic = {k: v.to(device) for k, v in enc_toxic.items()}\n",
    "\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "baseline = torch.full_like(enc_toxic[\"input_ids\"], PAD_TOKEN_ID).to(device)\n",
    "\n",
    "# Вычисление атрибуций\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=enc_toxic[\"input_ids\"],\n",
    "    baselines=baseline,\n",
    "    additional_forward_args=(enc_toxic[\"attention_mask\"],),\n",
    "    return_convergence_delta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e86690e0-e5a0-468f-96cb-46f05b3cda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 IG атрибуции по словам:\n",
      "  апсолютно    : -0.148\n",
      "  согласен     : -0.039\n",
      "  с            : -0.091\n",
      "  емельянычем  : +0.005\n",
      "  наши         : +0.078\n",
      "  12310        : -0.234\n",
      "  рублей       : -0.132\n",
      "  намного      : -0.089\n",
      "  круче        : -0.001\n",
      "  всей         : -0.057\n",
      "  этой         : +0.070\n",
      "  западной     : +0.001\n",
      "  херни        : +0.105\n",
      "  там          : +0.033\n",
      "  вокруг       : +0.044\n",
      "  одни         : +0.108\n",
      "  пидоы        : +1.948\n"
     ]
    }
   ],
   "source": [
    "attr_sum = attributions.squeeze(0).sum(dim=-1).cpu().detach().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(enc_toxic[\"input_ids\"][0])\n",
    "\n",
    "merged_tokens, merged_scores = [], []\n",
    "for tok, score in zip(tokens, attr_sum):\n",
    "    if tok in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "        continue\n",
    "    if tok.startswith(\"##\") and merged_tokens:\n",
    "        merged_tokens[-1] += tok[2:]\n",
    "        merged_scores[-1] += score\n",
    "    else:\n",
    "        merged_tokens.append(tok)\n",
    "        merged_scores.append(score)\n",
    "\n",
    "print(\"📊 IG атрибуции по словам:\")\n",
    "for tkn, sc in zip(merged_tokens, merged_scores):\n",
    "    print(f\"  {tkn:<12} : {sc:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd26bf3-25d5-4e14-8e5d-99f123ed860b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:resnet]",
   "language": "python",
   "name": "conda-env-resnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
